{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is the underlying concept of Support Vector Machines?\n",
    "The underlying concept of SVMs is to find the optimal hyperplane that separates two classes by maximizing the margin between them. SVM is a supervised learning algorithm that can be used for classification and regression."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What is the concept of a support vector?\n",
    "support vector is a data point that lies closest to the decision boundary(maximum margin hyperplane). Support vector play a critical role in defining the decision boundary, so it is called the SVM algorithm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. When using SVMs, why is it necessary to scale the inputs?\n",
    "\n",
    "Because SVMs are sensitive to the scale of the features. Features with large scales can dominate the optimization process. So it may not give proper results. So, it is necessary to scale the inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance?\n",
    "\n",
    "SVM classifier can output a confidence score. It is the distance between the decision boundary and the data point."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual form of the SVM problem?\n",
    "\n",
    "When training a model on a large dataset with millions of instances and hundreds of features, it is better to use the dual form of the SVM problem. The dual form is computationally more efficient and can handle a large number of features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Let&#39;s say you&#39;ve used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is it better to raise or lower (gamma)? What about the letter C?\n",
    "\n",
    "Using an RBF kernel to train an SVM classifier, if it appears to underfit the training collection,  it is better to increase gamma, which controls the width of the RBF kernel.\n",
    "\n",
    "If we decrease C, which controls the regularization strength, can also make the decision boundary more flexible and prevent overfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters (H, f, A, and b) be set?\n",
    "\n",
    "The QP parameters (H, f, A, and b) for the soft margin linear SVM classifier problem : H is the Hessian matrix, it is computed from the training data, f is a vector of zeros, A is a matrix that represents the constraints on the margin and can be computed from the training data; and b is a vector that represents the upper and lower bounds on the margin and can also be computed from the training data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and an SGDClassifier. See if you can get them to make a model that is similar to yours.\n",
    "\n",
    "LinearSVC and SVC models will produce similar results since they both solve a similar optimization problem to find the maximum margin hyperplane. \n",
    "\n",
    "SGDClassifier will have different accuracy , slightly low, since it uses different solver to optimize the loss function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. On the MNIST dataset, train an SVM classifier. You&#39;ll need to use one-versus-the-rest to assign all 10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want to tune the hyperparameters using small validation sets. What level of precision can you achieve?\n",
    "\n",
    "To tune the hyperparameters of the model, we can use a small validation set and try different values of the hyperparameters using grid search. The level of precision we can achieve depends on the choice of hyperparameters and the size and quality of the training and validation sets. \n",
    "\n",
    "With careful tuning and enough training data, we can achieve high levels of precision on the MNIST dataset with an SVM classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40590ebb26e95edc6c1f3a8a7137aad132b51444b5d516fe05a60311a56b75f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
