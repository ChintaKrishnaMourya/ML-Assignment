{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is the definition of a target function? In the sense of a real-life example, express the target function. How is a target function&#39;s fitness assessed?\n",
    "\n",
    "Target function is a function that maps input values to their output values. In real life sense, we can see in spam detection. Target function takes the emails as input and classify the outputs as spam or not. \n",
    "\n",
    "fitness is assessed by using respected tasks, for regression we use RMSE, MSE etc, for classification, we use Recall, Precision, F1 score, AUC ROC."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What are predictive models, and how do they work? What are descriptive types, and how do you use them? Examples of both types of models should be provided. Distinguish between these two forms of models.\n",
    "\n",
    "Predictive models are supervised learning models are used to predict the outcome, like regressions, classification. Predcitive models take input(labelled data) and give their corresponding output whereas descriptive models are unsupervised learning models are use to describe the data like finding the relations, groups within the data like clustering, association rule, PCA etc, we give unlabelled data and they find relations among the data points.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Describe the method of assessing a classification model&#39;s efficiency in detail. Describe the various measurement parameters.\n",
    "\n",
    "Assessing the classification models involves evaluating the performance of the model. We use Precision, Recall, F1 score, confusion matrix, ROC, AUC and accuracy. We have to take our matrics based on the data because, in an unbalanced data, finding accuracy gives biased information. \n",
    "\n",
    "Precision : out of total positives predicted, how many are truely positive.\n",
    "\n",
    "Recall : It is the ratio of truly predicted values out of actual positive values.\n",
    "\n",
    "F1 score : Harmoic mean of precision and recall, it is the trade off between them.\n",
    "\n",
    "Receiver Operating Characteristic (ROC) curve: It is the plot of TPR(recall) vs FPR(1- specificity) at different classification thresholds.\n",
    "\n",
    "Area Under the Curve (AUC): AUC is the area under ROC. It is the single metric that summarizes the overall performance of a classification model across all possible classification thresholds."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\n",
    "### i. In the sense of machine learning models, what is underfitting? What is the most common reason for underfitting?\n",
    "\n",
    "Underfitting means , our model performing not well on training set and also on test set. Model is not complex enough to capture underlying patterns in the data.\n",
    "\n",
    "Common reason is small dataset, not tuning the hyperparameters rigthly, or when the data is noisy or contains outliers.\n",
    "\n",
    "### ii. What does it mean to overfit? When is it going to happen?\n",
    "Overfitting means model performing rightly on train set but poorly on test set. Model is highly complex that it leanrt all the noise as patterns in the train set. when we use too many parameters, it can also lead to overfit.\n",
    "\n",
    "### iii. In the sense of model fitting, explain the bias-variance trade-off.\n",
    "\n",
    "It refers to the trade-off between the ability of a model to fit the training data (i.e., the bias) and its ability to generalize to new, unseen data (i.e., the variance). So in order to find balance between bias and variance, we use this bias- variance tradeoff."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Is it possible to boost the efficiency of a learning model? If so, please clarify how.\n",
    "\n",
    "Yes, it is possible to boost the effiency of the learning model. By selecting the right features that gives maximum information, by fine tunig the hyperparameters using grid search CV, and by scaling the data, etc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. How would you rate an unsupervised learning model&#39;s success? What are the most common success indicators for an unsupervised learning model?\n",
    "\n",
    "The success of an unsupervised learning model can be rated in different based on the our goals and application of the model. We can use clustering quality for clustering, and for dimnesionlaity reduction, we can assess it by degree of variance perserved in reducing data. And in visualsing , we can see how effectively the data points are separated."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Is it possible to use a classification model for numerical data or a regression model for categorical data with a classification model? Explain your answer.\n",
    "\n",
    "No , it is not possible to use a classification model for numerical data or a regression model for categorical data with a classification model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Describe the predictive modeling method for numerical values. What distinguishes it from categorical predictive modeling?\n",
    "\n",
    "Predictive modeling method for numerical values is a method to predict continuous output variable. We use regression analysis for that.  categorical predictive modeling is used to predict a categorical output variable based on one or more input variables. The difference between predictive modeling for numerical values and categorical predictive modeling is the type of output variable being predicted."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. The following data were collected when using a classification model to predict the malignancy of a group of patients&#39; tumors:\n",
    "i. Accurate estimates – 15 cancerous, 75 benign\n",
    "ii. Wrong predictions – 3 cancerous, 7 benign\n",
    "### Determine the model&#39;s error rate, Kappa value, sensitivity, precision, and F-measure.\n",
    "\n",
    "15 true positives (cancerous cases correctly predicted)\n",
    "3 false negatives (cancerous cases incorrectly predicted as benign)\n",
    "7 false positives (benign cases incorrectly predicted as cancerous)\n",
    "75 true negatives (benign cases correctly predicted)\n",
    "\n",
    "Kappa = (accuracy - expected accuracy) / (1 - expected accuracy)\n",
    "\n",
    "= Accuracy = (TP + TN) / (TP + FP + TN + FN) = (15 + 75) / (15 + 7 + 75 + 3) = 0.9\n",
    "\n",
    "expected accuracy = (TP+FP)/(TP+FP+TN+FN)= 0.32\n",
    "\n",
    "kappa = 0.696 / 69.6%\n",
    "\n",
    "ERROR RATE : (FP+FN)/(TP+FP+TN+FN)= 0.05\n",
    "\n",
    "Sensitivity : TP/TP+FN = 83.3%\n",
    "\n",
    "Precision : TP/(TP+FP)= 68.2%\n",
    "\n",
    "F- meausre : 2* (precision * recall)/(precision + recall)= 75%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Make quick notes on:\n",
    "\n",
    "\"The process of holding out\" : It refers to keeping a portion of data for testing and remaning data for training.\n",
    "\n",
    "\"Cross-validation by tenfold\" : It is a technique used to access the performance. It splits the data into 10 equal parts. It trains the model with 9 parts and tests on remaing one part. It is done 10 times with different parts for testing, and result is averaged to give estimate of model performance.\n",
    "\n",
    "\"Adjusting the parameters\" : It means selecting optimal values for parameters that influence the model's behavior . We typically do it by grid search."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Define the following terms:\n",
    "\"Purity vs. Silhouette width\" : Purity is clustering metric, how well cluster contains only a single class of points. It tanges from 0 to 1. Silhoutte width is also clustering metric , it measures how well a data point belongs to its assigned cluster. It ranges from -1 to +1. +1 means data point is well matched to its cluster.\n",
    "\n",
    "\"Boosting vs. Bagging\" : Both are ensemble methods. Boosting means, it combines multiple weak models into a strong model. It It works by sequentially training models and giving more weight to incorrectly classified data points to improve their classification accuracy in subsequent iterations.\n",
    "\n",
    "\"The eager learner vs. the lazy learner\" :Eager learner builds a model during training and uses it to predict during testing. Eg: Decision Trees. Lazy learner doesnt build model , it remembers the data and predcits during testing. EG : KNN algorithm"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
