{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What are the key reasons for reducing the dimensionality of a dataset? What are the major disadvantages?\n",
    "\n",
    "reducing the dimensionality can helps to reduce the amount of noise or redundancy in the data and time required to train and test ML models.\n",
    "\n",
    "Disadvantages : May lead to loss of important information or features in the data, risk of overfitting the model to the reduced dataset. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What is the dimensionality curse?\n",
    "As the number of features or dimensions in a dataset grows, the number of samples required to maintain the same level of accuracy also increases exponentially. This can lead to computational issues, overfitting, and poor generalization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tell if its possible to reverse the process of reducing the dimensionality of a dataset? If so, how can you go about doing it? If not, what is the reason?\n",
    "\n",
    "It is typically not possible to reverse the process of reducing the dimensionality of a dataset without some loss of information. The best approach is to store the original data and apply the reverse transformation to the reduced data if necessary.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Can PCA be utilized to reduce the dimensionality of a nonlinear dataset with a lot of variables?\n",
    "PCA is a linear technique, and it may not be effective at reducing the dimensionality of a nonlinear dataset. In such cases, nonlinear dimensionality reduction techniques such as kernel PCA can be effective.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Assume you&#39;re running PCA on a 1,000-dimensional dataset with a 95 percent explained variance ratio. What is the number of dimensions that the resulting dataset would have?\n",
    "The number of dimensions in the resulting dataset depends on the amount of explained variance ratio desired.\n",
    "\n",
    "Number of dimensions will be smaller than 1000 dimensions.\n",
    "\n",
    "The exact number of dimensions can be computed using the cumulative explained variance ratios.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Will you use vanilla PCA, incremental PCA, randomized PCA, or kernel PCA in which situations?\n",
    "\n",
    "Vanilla PCA is suitable for small to medium-sized datasets.\n",
    "\n",
    "Incremental PCA is useful for large datasets that don't fit in memory.\n",
    "\n",
    "Randomized PCA is ideal for very large datasets where computation time is an issue.\n",
    "\n",
    "Kernel PCA is appropriate for nonlinear datasets where linear methods like vanilla PCA won't work."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. How do you assess a dimensionality reduction algorithm&#39;s success on your dataset?\n",
    "\n",
    "The success of a dimensionality reduction algorithm can be assessed by measuring : amount of explained variance retained after the reduction, impact of the reduced dataset on the performance of a machine learning model trained on the original and reduced data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Is it logical to use two different dimensionality reduction algorithms in a chain?\n",
    "\n",
    "Using two different dimensionality reduction algorithms in a chain may be useful in certain cases where the first algorithm is unable to capture all the relevant information or the second algorithm can refine the output of the first algorithm.\n",
    "\n",
    "However, it also increases the complexity of the model and may not always be necessary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40590ebb26e95edc6c1f3a8a7137aad132b51444b5d516fe05a60311a56b75f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
