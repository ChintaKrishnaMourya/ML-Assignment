{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Recognize the differences between supervised, semi-supervised, and unsupervised learning.\n",
    "Supervised learning : Model trained on labelled dataset.\n",
    "\n",
    "Semi -supervised learning : model is trained on a combination of labeled and unlabeled data.\n",
    "\n",
    "Unsupervised learning : Model trained on unlabeled data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Describe in detail any five examples of classification problems.\n",
    "Spam filtering sentiment analysis, image recognition, heart risk prediction, credit risk prediction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Describe each phase of the classification process in detail.\n",
    "Data preparation: Collecting, cleaning, and transforming the data so that it can be used for training a model\n",
    "\n",
    "Feature extraction: Selecting relevant features from the data that will be used as inputs to the model\n",
    "\n",
    "Model training: Using the prepared data to train a model to accurately classify new data\n",
    "\n",
    "Model evaluation: Testing the accuracy of the trained model on a separate set of test data to measure its performance\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Go through the SVM model in depth using various scenarios.\n",
    "\n",
    "SVM can be used in scenarios such as image classification, text classification, etc. SVM works by finding a hyperplane in a high-dimensional space to separate the data into classes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Go over the kNN model in depth.\n",
    "\n",
    "KNN algorithm works by calculating the distance between the new data point and all of the labeled data points, and then selecting the k nearest neighbors to determine the class of the new data point. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Discuss the kNN algorithm's error rate and validation error.\n",
    "\n",
    "error rate is the rate at which the model misclassifies data points.\n",
    "\n",
    "validation error is the rate at which the model misclassifies new data points that were not used in the training set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. For kNN, talk about how to measure the difference between the test and training results.\n",
    "\n",
    "We can use accuracy, precision, recall, and F1 score to measure the difference between the test and training results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. What is a decision tree, exactly? What are the various kinds of nodes? Explain all in depth.\n",
    "A decision tree is a type of model used for classification or regression tasks. It splits the data  based on the value of a feature until a stopping criterion is met. \n",
    "\n",
    "Root node: top node in the tree\n",
    "\n",
    "decision node : a node that represents the split on a feature.\n",
    "\n",
    "Leaf node : A node that represents the final prediction.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Describe the different ways to scan a decision tree.\n",
    "\n",
    "depth-first search (DFS) and breadth-first search (BFS).\n",
    "\n",
    "DFS follows a path from the root node to the leaf node, BFS scans each level of the tree before proceeding to the next level."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Describe in depth the decision tree algorithm.\n",
    "\n",
    "decision tree algorithm builds a tree-like model from a set of labeled training data.It starts from Root node by selecting most informative feature and recursively splits the data into subsets based on the values of the feature until it reaches the leaf nodes, which contain the final decision or classification.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. In a decision tree, what is inductive bias? What would you do to stop overfitting?\n",
    "In decision tree learning, the inductive bias is the set of assumptions that the decision tree uses to generalize from the training data to the unseen test data.\n",
    "\n",
    "To stop overfitting, we can do pruning. Pruning removes nodes from the tree that do not improve the overall accuracy.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.Explain advantages and disadvantages of using a decision tree?\n",
    "advantages : can handle both categorical and continuous data,can handle missing data by imputing the missing values, easy to interpret and visualize.\n",
    "\n",
    "disadvantages : can easily overfit the training data, sensitive to small variations in the data, which can lead to different trees and inconsistent results, \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Describe in depth the problems that are suitable for decision tree learning.\n",
    "We can use decision trees in binary classification, and multi class classification, regression, and also data exploration, feature selection.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Describe in depth the random forest model. What distinguishes a random forest?\n",
    "Random Forest Model is an ensenble learning model. It builds multiple decision trees and trains them with random samples selected from dataset. And finally aggregate the outcome value.Because of multiple trees, Random Forest builds a model that has low bias and variance. And it can also effectively tackle missing data and outliers because of multiple samples of data and multiple decision trees.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. In a random forest, talk about OOB error and variable value.\n",
    "\n",
    "out-of-bag (OOB) error estimate is used to calculate the prediction accuracy of a random forest model. It is the average classification error for each observation using only the trees that did not include that observation in their bootstrap sample.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40590ebb26e95edc6c1f3a8a7137aad132b51444b5d516fe05a60311a56b75f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
